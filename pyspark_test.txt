import pyspark 
sc = pyspark.SparkContext('local[*]')

# do something to prove it works
rdd = sc.parallelize([1,2,3,4,5])
rddCollect = rdd.collect()
print("Number of Partitions: "+str(rdd.getNumPartitions()))
print("Action: First element: "+str(rdd.first()))
print(rddCollect)